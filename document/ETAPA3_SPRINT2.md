# ü§ñ Etapa 3 ‚Äì Constru√ß√£o do Modelo de IA

## 1Ô∏è‚É£ Sele√ß√£o do Modelo

Para o desenvolvimento do modelo de previs√£o da produtividade agr√≠cola da cana-de-a√ß√∫car, avaliamos algoritmos de aprendizado de m√°quina adequados para dados tabulares multivariados e que pudessem se beneficiar do uso de **GPU com CUDA**.

### üéØ Requisitos t√©cnicos considerados:

- Dados de entrada com m√∫ltiplas vari√°veis num√©ricas mensais (NDVI, chuva, temperatura, m√™s).
- Quantidade moderada de dados (n√£o massivos, mas com sazonalidade importante).
- Modelo com **baixo tempo de treinamento** e **capacidade de capturar rela√ß√µes n√£o-lineares** entre clima, vegeta√ß√£o e produtividade.
- Possibilidade de execu√ß√£o acelerada por **GPU NVIDIA com CUDA**, dispon√≠vel em nossa infraestrutura com WSL2.

---

## üîç Modelo escolhido: **XGBoost com acelera√ß√£o por GPU**

Selecionamos o algoritmo **XGBoost Regressor** com o par√¢metro `tree_method='gpu_hist'`, que permite:

- Treinamento extremamente r√°pido utilizando n√∫cleos CUDA;
- Performance compar√°vel ou superior ao Random Forest em muitos cen√°rios com poucos dados;
- Capacidade de realizar **interpreta√ß√£o dos resultados** por meio da import√¢ncia das vari√°veis.

### ‚öôÔ∏è Configura√ß√£o inicial do modelo:

```python
from xgboost import XGBRegressor

modelo = XGBRegressor(
    tree_method='gpu_hist',    # Ativa o uso da GPU (via CUDA)
    n_estimators=200,
    max_depth=6,
    learning_rate=0.1,
    random_state=42
)


## 2Ô∏è‚É£ Treinamento e Avalia√ß√£o do Modelo

Com o modelo XGBoost configurado para utilizar **acelera√ß√£o via GPU (CUDA)**, realizamos o treinamento utilizando os dados hist√≥ricos de NDVI, clima e produtividade.

---

### üîÄ Divis√£o dos dados

A base consolidada foi dividida em dois subconjuntos:

- **80% dos dados** para treinamento (`X_train`, `y_train`)
- **20% dos dados** para teste e valida√ß√£o do modelo (`X_test`, `y_test`)

```python
from sklearn.model_selection import train_test_split

X = df[['NDVI', 'Chuva (mm)', 'Temp. M√°x. (C)', 'Temp. M√≠n. (C)', 'Mes']]
y = df['Produtividade (ton/ha)']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


## üìä Comparativo entre Modelos: XGBoost vs SVR

Testamos dois algoritmos para prever a produtividade da cana-de-a√ß√∫car:

| Modelo     | MAE (ton/ha) | MSE | R¬≤ (Coef. Determina√ß√£o) |
|------------|--------------|-----|--------------------------|
| XGBoost GPU| 1.22         | 2.08| **-0.53**                |
| SVR (RBF)  | **1.08**     | **1.37** | **-0.01**           |

‚úÖ **O modelo SVR apresentou melhor desempenho** geral, com menor erro absoluto e quadr√°tico.  
üìâ O R¬≤ ainda est√° abaixo de zero, indicando que o modelo n√£o generaliza bem, mas √© **muito superior ao XGBoost neste cen√°rio de poucos dados com baixa varia√ß√£o**.

---

### üéØ Justificativa da Escolha do SVR

O **SVR com kernel RBF** foi escolhido por sua capacidade de lidar com:

- Pequenas bases de dados;
- Rela√ß√µes n√£o lineares entre as vari√°veis clim√°ticas e a produtividade;
- Robustez contra overfitting em datasets com pouca variabilidade.

Al√©m disso, o uso de `StandardScaler` para normaliza√ß√£o das vari√°veis foi essencial para que o SVR tivesse bom desempenho.

---

### üìà Visualiza√ß√µes

- **Gr√°fico Real vs Previsto (SVR)**: mostra boa tend√™ncia de ajuste mesmo com dados limitados.
- Arquivo salvo: `../tests/images/svr_real_vs_prevista.png`


